# ğŸ–¼ï¸ Image Caption Generator

Generate natural language descriptions for images using a deep learning model with an Encoder-Decoder architecture (CNN-RNN).

---

## ğŸ“Œ Table of Contents

* [Overview](#overview)
* [Architecture](#architecture)
* [Project Structure](#project-structure)
* [How It Works](#how-it-works)
* [Setup Instructions](#setup-instructions)
* [Training](#training)
* [Inference](#inference)
* [Evaluation](#evaluation)
* [Sample Output](#sample-output)
* [Dependencies](#dependencies)
* [Data Management](#data-management)

---

## ğŸ§  Overview

This project takes an image as input and outputs a relevant caption describing the contents of the image. It uses:

* **CNN** (ResNet50 or InceptionV3) for feature extraction (Encoder)
* **LSTM** for generating captions (Decoder)
* **Tokenizer** to map words to integer sequences
* **Custom DataLoader** using TensorFlow's `tf.data`

---

## ğŸ—ï¸ Architecture

### Encoder (CNN)

* Pre-trained CNN (e.g. InceptionV3)
* Removes final classification layer
* Outputs a 2048-dim feature vector

### Decoder (RNN)

* Embedding Layer
* LSTM with Attention
* Fully Connected Layer to predict vocabulary word at each time step

---

## ğŸ“ Project Structure

```
image_caption_generator/
â”œâ”€â”€ data/                         # Not pushed to GitHub (see Data Management)
â”‚   â”œâ”€â”€ captions.txt              # Raw MSCOCO-style captions
â”‚   â”œâ”€â”€ captions_clean.csv        # Preprocessed captions with <start> and <end>
â”‚   â”œâ”€â”€ tokenizer.pkl             # Saved tokenizer
â”‚   â””â”€â”€ features/                 # Pre-extracted .npy feature files
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ encoder.py
â”‚   â”œâ”€â”€ decoder.py
â”‚   â”œâ”€â”€ dataloader.py
â”‚   â”œâ”€â”€ train.py
â”‚   â”œâ”€â”€ utils.py
â”‚   â”œâ”€â”€ tokenizer_builder.py
â”‚   â”œâ”€â”€ config.py
â”‚   â”œâ”€â”€ extract_features.py
â”‚   â””â”€â”€ preprocessing_captions.py
â”‚
â”œâ”€â”€ inference.py
â”œâ”€â”€ inference_utils.py
â”œâ”€â”€ evaluate.py
â”œâ”€â”€ test.py
â”œâ”€â”€ test_dataloader.py
â”œâ”€â”€ check_tokenizer.py
â”œâ”€â”€ force_fix_captions.py
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

---

## âš™ï¸ How It Works

1. **Preprocess captions:** Clean and add `<start>` / `<end>` tokens
2. **Build tokenizer:** Fit on captions and save `tokenizer.pkl`
3. **Extract image features:** Use InceptionV3 to save `.npy` files
4. **Load Dataset:** Map captions to image features
5. **Train model:** Pass image + previous words into encoder/decoder
6. **Evaluate or infer:** Decode predicted words into caption

---

## ğŸ§ª Setup Instructions

1. **Clone repo**

```bash
git clone https://github.com/bhagavath12/image_caption_generator.git
cd image_caption_generator
```

2. **Install dependencies**

```bash
pip install -r requirements.txt
```

3. **Preprocess Captions**

```bash
python src/preprocessing_captions.py
# OR to forcibly fix missing tokens:
python force_fix_captions.py
```

4. **Build Tokenizer**

```bash
python src/tokenizer_builder.py
```

5. **Extract Image Features**

```bash
python src/extract_features.py
```

6. **Test Dataloader**

```bash
python test_dataloader.py
```

---

## ğŸ‹ï¸ Training

```bash
python src/train.py
```

Checkpoints will be saved to:

```
checkpoints/train/
```

---

## ğŸ§  Inference

```bash
python inference.py --image test.jpg
```

Or use the test wrapper:

```bash
python test.py
```

Make sure `test.jpg` is placed in the root directory.

---

## ğŸ“Š Evaluation

```bash
python evaluate.py
```

Returns BLEU score and optionally attention visualization.

---

## ğŸ–¼ï¸ Sample Output

```
Input: ğŸ¶ test.jpg
Output: "A brown dog is running through the grass."
```

---

## ğŸ“¦ Dependencies

```txt
tensorflow>=2.10
numpy
pandas
matplotlib
pickle
opencv-python
scikit-learn
```

Install via:

```bash
pip install -r requirements.txt
```

---

## ğŸ“ Data Management

The `data/` folder is **intentionally excluded** from GitHub to avoid:

* Uploading large `.npy` feature files
* Uploading raw image datasets
* Exceeding GitHubâ€™s 100MB file limit

You must manually create the `data/` folder:

```
data/
â”œâ”€â”€ captions.txt
â”œâ”€â”€ captions_clean.csv
â”œâ”€â”€ tokenizer.pkl
â”œâ”€â”€ features/         # Contains *.npy files per image
â””â”€â”€ images/           # Optional: original images
```

Ensure you generate required files using:

* `force_fix_captions.py`
* `tokenizer_builder.py`
* `extract_features.py`

---

## ğŸ’¡ Notes

* BLEU scores may vary based on beam search, epochs, and data size
* Add `beam search` for better inference performance
* Preprocessing and tokenizer consistency are crucial

---

## ğŸ“® Credits

* MSCOCO Dataset
* TensorFlow Tutorials
* Show, Attend and Tell (Xu et al.)

---

## ğŸ› ï¸ Improvements You Can Add

* ğŸ” Attention Visualization
* ğŸ§  Beam Search Decoding
* ğŸ“¦ PyTorch version
* ğŸ“ˆ Gradio/Web demo

---

## ğŸ“¬ License

MIT License
